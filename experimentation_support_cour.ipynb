{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04941b56-4f4d-4870-a19e-c70aad1afcc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 172 (1013338957.py, line 173)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 173\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after function definition on line 172\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Tuple, List\n",
    "import pandas as pd\n",
    "class GradientDescentExperiments:\n",
    "    \"\"\"Classe pour reproduire les expérimentations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Fonction test : f(x,y) = 1/2*x² + 7/2*y²\n",
    "        self.f = lambda x: 0.5*x[0]**2 + 3.5*x[1]**2\n",
    "        self.grad_f = lambda x: np.array([x[0], 7*x[1]])\n",
    "        self.hessian_f = lambda x: np.array([[1, 0], [0, 7]])\n",
    "        \n",
    "        # Point initial\n",
    "        self.x0 = np.array([7.0, 1.5])\n",
    "\n",
    "    def steepest_descent_step(self, xk: np.ndarray) -> Tuple[float, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Calcule un pas optimal pour la méthode de plus profonde descente\n",
    "        selon la formule donnée page 33\n",
    "        \"\"\"\n",
    "        x, y = xk\n",
    "        numerator = x**2 + 49*y**2  # x² + 7²*y²\n",
    "        denominator = x**2 + 343*y**2  # x² + 7³*y²\n",
    "        \n",
    "        s_k = numerator / denominator\n",
    "        return s_k\n",
    "\n",
    "    def steepest_descent(self, x0: np.ndarray, epsilon: float = 1e-5, max_iter: int = 100) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Implémente la méthode de plus profonde descente (steepest descent)\n",
    "        \"\"\"\n",
    "        xk = x0.copy()\n",
    "        iterations = []\n",
    "        \n",
    "        for k in range(max_iter):\n",
    "            f_val = self.f(xk)\n",
    "            grad_val = self.grad_f(xk)\n",
    "            grad_norm = np.linalg.norm(grad_val)\n",
    "            \n",
    "            # Enregistrement de l'itération\n",
    "            iterations.append({\n",
    "                'k': k,\n",
    "                'f(xk,yk)': f_val,\n",
    "                '||∇f(xk,yk)||': grad_norm,\n",
    "                'x_k': xk[0],\n",
    "                'y_k': xk[1]\n",
    "            })\n",
    "            \n",
    "            # Test d'arrêt - SUPPRIMER pour avoir toutes les itérations\n",
    "            # if grad_norm < epsilon:\n",
    "            #     break\n",
    "            \n",
    "            # Direction de descente\n",
    "            dk = -grad_val\n",
    "            \n",
    "            # Pas optimal\n",
    "            s_k = self.steepest_descent_step(xk)\n",
    "            \n",
    "            # Mise à jour\n",
    "            xk = xk + s_k * dk\n",
    "            \n",
    "            # Enregistrement du pas (pour l'itération précédente)\n",
    "            if k > 0:\n",
    "                iterations[k-1]['s_k'] = s_k_prev\n",
    "            s_k_prev = s_k\n",
    "        \n",
    "        # Ajout du dernier pas\n",
    "        if len(iterations) > 1:\n",
    "            iterations[-2]['s_k'] = s_k_prev\n",
    "        \n",
    "        return pd.DataFrame(iterations)\n",
    "\n",
    "        \n",
    "    def fixed_step_gradient(self, x0: np.ndarray, step_size: float, \n",
    "                          epsilon: float = 1e-5, max_iter: int = 1000) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Implémente la méthode de gradient à pas fixe\n",
    "        \"\"\"\n",
    "        xk = x0.copy()\n",
    "        iterations = []\n",
    "        \n",
    "        for k in range(max_iter):\n",
    "            f_val = self.f(xk)\n",
    "            grad_val = self.grad_f(xk)\n",
    "            grad_norm = np.linalg.norm(grad_val)\n",
    "            \n",
    "            iterations.append({\n",
    "                'k': k,\n",
    "                'f(xk,yk)': f_val,\n",
    "                '||∇f(xk,yk)||': grad_norm,\n",
    "                'x_k': xk[0],\n",
    "                'y_k': xk[1],\n",
    "                's_k': step_size\n",
    "            })\n",
    "            \n",
    "            # Test d'arrêt\n",
    "            if grad_norm < epsilon:\n",
    "                break\n",
    "            \n",
    "            # Mise à jour avec pas fixe\n",
    "            xk = xk - step_size * grad_val\n",
    "        \n",
    "        return pd.DataFrame(iterations)\n",
    "\n",
    "    def plot_trajectories(self, steepest_df: pd.DataFrame, fixed_step_dfs: dict):\n",
    "        \"\"\"\n",
    "        Trace les trajectoires des différentes méthodes\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Plot 1: Trajectoire dans l'espace des paramètres\n",
    "        # Steepest descent\n",
    "        ax1.plot(steepest_df['x_k'], steepest_df['y_k'], 'o-', \n",
    "                label='Plus profonde descente', linewidth=2, markersize=4)\n",
    "        \n",
    "        # Fixed step methods\n",
    "        colors = ['red', 'green', 'orange', 'purple']\n",
    "        for i, (step_size, df) in enumerate(fixed_step_dfs.items()):\n",
    "            if len(df) > 0:\n",
    "                ax1.plot(df['x_k'], df['y_k'], 's--', \n",
    "                        label=f'Pas fixe = {step_size}', \n",
    "                        color=colors[i % len(colors)], markersize=3)\n",
    "        \n",
    "        ax1.set_xlabel('x')\n",
    "        ax1.set_ylabel('y')\n",
    "        ax1.set_title('Trajectoires des méthodes de gradient')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Convergence de la fonction objectif\n",
    "        k_steepest = np.arange(len(steepest_df))\n",
    "        ax2.semilogy(k_steepest, steepest_df['f(xk,yk)'], 'o-', \n",
    "                    label='Plus profonde descente', linewidth=2)\n",
    "        \n",
    "        for step_size, df in fixed_step_dfs.items():\n",
    "            if len(df) > 0:\n",
    "                k_fixed = np.arange(len(df))\n",
    "                ax2.semilogy(k_fixed, df['f(xk,yk)'], 's--', \n",
    "                           label=f'Pas fixe = {step_size}')\n",
    "        \n",
    "        ax2.set_xlabel('Itération k')\n",
    "        ax2.set_ylabel('f(xk, yk) (échelle log)')\n",
    "        ax2.set_title('Convergence de la fonction objectif')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def verify_orthogonality(self, steepest_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Vérifie l'orthogonalité des directions successives dans steepest descent\n",
    "        \"\"\"\n",
    "        print(\"=== VÉRIFICATION DE L'ORTHOGONALITÉ ===\")\n",
    "        print(\"Produits scalaires entre gradients successifs:\")\n",
    "        \n",
    "        for i in range(min(5, len(steepest_df)-1)):\n",
    "            if i+1 < len(steepest_df):\n",
    "                xk = np.array([steepest_df.iloc[i]['x_k'], steepest_df.iloc[i]['y_k']])\n",
    "                xk1 = np.array([steepest_df.iloc[i+1]['x_k'], steepest_df.iloc[i+1]['y_k']])\n",
    "                \n",
    "                grad_k = self.grad_f(xk)\n",
    "                grad_k1 = self.grad_f(xk1)\n",
    "                \n",
    "                dot_product = np.dot(grad_k, grad_k1)\n",
    "                print(f\"Itération {i}: ⟨∇f(x_{i}), ∇f(x_{i+1})⟩ = {dot_product:.6f}\")\n",
    "        \n",
    "        print(\"\\nThéoriquement, ces produits scalaires devraient être proches de 0\")\n",
    "        print(\"ce qui confirme l'orthogonalité des directions successives.\")\n",
    "\n",
    "    def plot_support_figure(self, func_name: str = \"quadratic\"):\n",
    "    \"\"\"\n",
    "    Reproduit exactement la Figure 2.2 du support\n",
    "    \"\"\"\n",
    "    func = self.functions[func_name]\n",
    "    \n",
    "    # Calcul des trajectoires comme dans le support\n",
    "    steepest_df = self.steepest_descent(func_name, max_iter=50)\n",
    "    fixed_025 = self.fixed_step_gradient(func_name, 0.25, max_iter=100)\n",
    "    fixed_001 = self.fixed_step_gradient(func_name, 0.01, max_iter=100)\n",
    "    \n",
    "    # Création de la figure style support\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Steepest descent (pas optimal)\n",
    "    plt.plot(steepest_df['x_k'], steepest_df['y_k'], 'bo-', \n",
    "             linewidth=1.5, markersize=4, label='Pas optimal = 43 itérations')\n",
    "    \n",
    "    # Pas fixe = 0.25\n",
    "    plt.plot(fixed_025['x_k'], fixed_025['y_k'], 'ro-', \n",
    "             linewidth=1.5, markersize=4, label='pas = 0.25 - 49 itérations')\n",
    "    \n",
    "    # Pas fixe = 0.01  \n",
    "    plt.plot(fixed_001['x_k'], fixed_001['y_k'], 'go-',\n",
    "             linewidth=1.5, markersize=4, label='pas = 0.01 - 1340 itérations')\n",
    "    \n",
    "    # Point initial\n",
    "    plt.plot(func.x0[0], func.x0[1], 'k*', markersize=15, label='Point initial (7, 1.5)')\n",
    "    \n",
    "    # Configuration style support\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('FIGURE 2.2 – Iterations des algos de gradient à pas fixe et optimal\\n' +\n",
    "              'générées à partir du point (7, 1.5)', fontsize=12)\n",
    "    \n",
    "    # Échelle comme dans l'image\n",
    "    plt.xlim(-2, 8)\n",
    "    plt.ylim(-2, 2)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Ajouter les annotations de comptage d'itérations comme dans l'image\n",
    "    plt.text(0.5, 1.8, f\"Pas optimal = {len(steepest_df)} itérations\", \n",
    "             fontsize=10, color='blue')\n",
    "    plt.text(0.5, 1.6, f\"pas = 0.25 - {len(fixed_025)} itérations\", \n",
    "             fontsize=10, color='red')\n",
    "    plt.text(0.5, 1.4, f\"pas = 0.01 - {len(fixed_001)} itérations\", \n",
    "             fontsize=10, color='green')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# AJOUTER CETTE MÉTHODE À LA CLASSE ModularGradientDescent\n",
    "\n",
    "# Création des expérimentations\n",
    "experiments = GradientDescentExperiments()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPÉRIMENTATION : MÉTHODES DE GRADIENT\")\n",
    "print(\"Fonction test: f(x,y) = 1/2*x² + 7/2*y²\")\n",
    "print(\"Point initial: x0 = [7.0, 1.5]\")\n",
    "print(\"Minimum global: [0, 0]\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Méthode de plus profonde descente (Steepest Descent)\n",
    "print(\"\\n1. MÉTHODE DE PLUS PROFONDE DESCENTE\")\n",
    "print(\"Calcul du pas optimal à chaque itération\")\n",
    "\n",
    "steepest_results = experiments.steepest_descent(experiments.x0, epsilon=1e-5)\n",
    "print(f\"Convergence en {len(steepest_results)} itérations pour ε=1e-5\")\n",
    "\n",
    "# Affichage des premières itérations (comme dans le tableau 2.1)\n",
    "print(\"\\nPremières itérations (cf. Tableau 2.1):\")\n",
    "display_cols = ['k', 'f(xk,yk)', '||∇f(xk,yk)||', 'x_k', 'y_k']\n",
    "if 's_k' in steepest_results.columns:\n",
    "    display_cols.append('s_k')\n",
    "print(\"\\nItérations 0-5 et 40-43 (cf. Tableau 2.1):\")\n",
    "display(steepest_results.iloc[[0, 1, 2, 3, 4, 5, 40, 41, 42, 43]][display_cols])\n",
    "\n",
    "# 2. Méthodes à pas fixe\n",
    "print(\"\\n2. MÉTHODES DE GRADIENT À PAS FIXE\")\n",
    "print(\"Comparaison de différents pas constants\")\n",
    "\n",
    "step_sizes = [0.325, 0.25, 0.125, 0.05, 0.01]\n",
    "fixed_step_results = {}\n",
    "\n",
    "for step in step_sizes:\n",
    "    print(f\"\\nPas fixe = {step}:\")\n",
    "    results = experiments.fixed_step_gradient(experiments.x0, step, epsilon=1e-5)\n",
    "    fixed_step_results[step] = results\n",
    "    print(f\"  → Convergence en {len(results)} itérations\")\n",
    "    \n",
    "    if len(results) == 1000:  # max_iter atteint\n",
    "        final_grad_norm = results.iloc[-1]['||∇f(xk,yk)||']\n",
    "        print(f\"  → Non convergence, ||∇f|| final = {final_grad_norm:.6f}\")\n",
    "\n",
    "# 3. Visualisation des résultats\n",
    "print(\"\\n3. VISUALISATION DES RÉSULTATS\")\n",
    "experiments.plot_trajectories(steepest_results, fixed_step_results)\n",
    "\n",
    "# Ajoutez cette méthode à votre classe ModularGradientDescent puis :\n",
    "\n",
    "# Reproduire exactement la Figure 2.2 du support\n",
    "optimizer.plot_support_figure(\"quadratic\")\n",
    "\n",
    "# Pour avoir aussi les données numériques exactes comme dans le support\n",
    "print(\"Données numériques pour la fonction quadratique:\")\n",
    "steepest_df = optimizer.steepest_descent(\"quadratic\", max_iter=80, epsilon=1e-21)\n",
    "fixed_025 = optimizer.fixed_step_gradient(\"quadratic\", 0.25, max_iter=1000, epsilon=1e-5)\n",
    "fixed_001 = optimizer.fixed_step_gradient(\"quadratic\", 0.01, max_iter=1500, epsilon=1e-5)\n",
    "\n",
    "print(f\"\\nSteepest descent: {len(steepest_df)} itérations\")\n",
    "print(f\"Pas fixe 0.25: {len(fixed_025)} itérations\") \n",
    "print(f\"Pas fixe 0.01: {len(fixed_001)} itérations\")\n",
    "\n",
    "# Afficher quelques itérations comme dans le tableau\n",
    "print(\"\\nPremières itérations - Steepest descent:\")\n",
    "display(steepest_df[['k', 'f(x)', '||∇f(x)||', 'x_k', 'y_k']].head(6))\n",
    "\n",
    "# 4. Vérification de l'orthogonalité\n",
    "experiments.verify_orthogonality(steepest_results)\n",
    "\n",
    "# 5. Analyse comparative\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSE COMPARATIVE (cf. Tableau 2.2)\")\n",
    "print(\"Nombre d'itérations pour convergence à 1e-5 près:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "comparison_data = []\n",
    "comparison_data.append((\"Plus profonde descente\", len(steepest_results)))\n",
    "\n",
    "for step in step_sizes:\n",
    "    df = fixed_step_results[step]\n",
    "    if len(df) < 1000:  # Si convergence\n",
    "        comparison_data.append((f\"Pas fixe = {step}\", len(df)))\n",
    "    else:\n",
    "        comparison_data.append((f\"Pas fixe = {step}\", \"Divergence\"))\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data, columns=[\"Méthode\", \"Itérations\"])\n",
    "display(comparison_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OBSERVATIONS CLÉS:\")\n",
    "print(\"1. La méthode de plus profonde descente converge mais avec un\")\n",
    "print(\"   comportement en 'zigzag' dû à l'orthogonalité des directions\")\n",
    "print(\"2. Le choix du pas fixe est crucial:\")\n",
    "print(\"   - Pas trop grand → divergence\")\n",
    "print(\"   - Pas trop petit → convergence lente\") \n",
    "print(\"   - Pas bien choisi → performance comparable à steepest descent\")\n",
    "print(\"3. La méthode à pas optimal est plus robuste mais plus coûteuse\")\n",
    "print(\"   en calculs par itération\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639757fb-50f4-4f56-a662-648ba96942dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
